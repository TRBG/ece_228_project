{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fish Classification Code\n",
    "The code loads the data.npy and data_aug.npy files that contain the dataset and augmented dataset to train and test the model on. the files are in the subfolder '/data/'\n",
    "\n",
    "This code should be run on a GPU (it was implemented in the datahub - py37-tf2 kernel).\n",
    "It implements the original model proposed by Rathi et al. (https://ieeexplore.ieee.org/document/8593044). then it implements our improved model after that in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import cv2 as cv \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sb\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Support Packages\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For The Data Hub\n",
    "# Loading the dataset (both original and augmented) from the npy files\n",
    "x_train_orig = np.load('data/data.npy')\n",
    "x_train_aug = np.load('data/data_aug.npy')\n",
    "\n",
    "labels_orig = np.load('data/species.npy')\n",
    "labels_aug = np.load('data/species_aug.npy')\n",
    "\n",
    "x_train = np.concatenate((x_train_orig, x_train_aug), axis=0)\n",
    "labels = np.concatenate((labels_orig, labels_aug), axis=0)\n",
    "\n",
    "labels = labels.astype(np.uint8)\n",
    "labels = labels - 1\n",
    "print(labels.dtype)\n",
    "\n",
    "# to test the original dataset without augmentation (uncomment the following two lines): \n",
    "# x_train = x_train_orig \n",
    "# labels = labels_orig\n",
    "\n",
    "# Deleting extra variables to free up memory\n",
    "del x_train_orig\n",
    "del x_train_aug\n",
    "\n",
    "del labels_orig\n",
    "del labels_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training, validation, and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training, validation, and testing.\n",
    "\n",
    "X_train_init, X_test, Y_train_init, Y_test = train_test_split(x_train, labels, train_size=0.8, random_state=42, shuffle=True)\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train_init, Y_train_init, train_size=0.8, random_state=42, shuffle=True)\n",
    "\n",
    "# Deleting extra variables to free up memory\n",
    "del X_train_init\n",
    "del Y_train_init\n",
    "\n",
    "del labels\n",
    "del x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing the original CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reproduces the code as listed in Rathi et al. (https://ieeexplore.ieee.org/document/8593044). The initial dataset comes from: http://groups.inf.ed.ac.uk/f4k/GROUNDTRUTH/RECOG/. To prepare the data for this code scripts 1, 2, and 3 must be implemented before running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The architecture of the model:\n",
    "\n",
    "def fish_classification_nn():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(100, 100, 4), padding = 'same')) \n",
    "    model.add(layers.MaxPooling2D((5, 5), padding = 'same'))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), activation='relu', padding = 'same'))\n",
    "    model.add(layers.MaxPooling2D((5, 5), padding = 'same'))\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (5, 5), activation='relu', padding = 'same'))\n",
    "    model.add(layers.MaxPooling2D((5, 5), padding = 'same'))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(200, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2)) \n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2)) \n",
    "    model.add(layers.Dense(23, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Generating the summary of the model\n",
    "rathi_fish_class=fish_classification_nn()\n",
    "rathi_fish_class.summary()\n",
    "\n",
    "#Compile and train the model\n",
    "rathi_fish_class.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "rathi_fish_class_history = rathi_fish_class.fit(X_train, Y_train, validation_data= (X_validation, Y_validation),\n",
    "                                                epochs= 100 , batch_size= 10, shuffle=True)\n",
    "\n",
    "# Plotting the accuracy and loss for the training and validation data (vs. EPOCH)\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('The Accuracy Vs. Epoch No.', fontsize= 14)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(rathi_fish_class_history.history['val_accuracy'])\n",
    "plt.plot(rathi_fish_class_history.history['accuracy'])\n",
    "plt.ylabel('Accuracy', fontsize= 14)\n",
    "plt.xlabel('Epoch No.', fontsize= 14)\n",
    "plt.title('The Accuracy Vs. Epoch No.', fontsize= 14)\n",
    "plt.legend(('Validation Set Accuracy','Training Set Accuracy'), fontsize= 14, loc=4)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(rathi_fish_class_history.history['val_loss'])\n",
    "plt.plot(rathi_fish_class_history.history['loss'])\n",
    "plt.ylabel('Loss', fontsize= 14)\n",
    "plt.xlabel('Epoch No.', fontsize= 14)\n",
    "plt.title('The Loss Vs. Epoch No.', fontsize= 14)\n",
    "plt.legend(('Validation Set Loss','Training Set Loss'), fontsize= 14, loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on the testing dataset\n",
    "We evaluate the model on the testing dataset as well as generate the confusion matrix (normalized) and classification report ( with precision, recall, f1-score, and support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predicting the classes of the testing dataset\n",
    "Y_pred = rathi_fish_class.predict_classes(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "con_mat = confusion_matrix(Y_test,Y_pred)\n",
    "\n",
    "#normalize result from 0 to 1:\n",
    "sum_of_classes = con_mat.sum(axis=1)\n",
    "con_mat_norm = np.zeros((23,23))\n",
    "for i in range(con_mat.shape[0]):\n",
    "    \n",
    "    if sum_of_classes[i] != 0:\n",
    "        con_mat_norm[i,:] = np.around(con_mat.astype('float')[i,:] / sum_of_classes[i], decimals=2) \n",
    "# the above code rounds to the first 2 decimals and then divides by the sum of every col \n",
    "\n",
    "classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]\n",
    "\n",
    "con_mat_df = pd.DataFrame(con_mat_norm, index = classes, columns = classes)\n",
    " \n",
    "figure = plt.figure(figsize=(15,10))\n",
    "sb.set(font_scale = 1.3)\n",
    "sb.heatmap(con_mat_norm, annot=True, cmap='inferno', annot_kws={\"size\": 12},  square=True)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()\n",
    "\n",
    "## Test set accuracy\n",
    "accuracy = 1 - np.count_nonzero((Y_test[:,0] - Y_pred))/len(Y_pred)\n",
    "print(\"The Accuracy for Rathi's Model is: %.2f%%\" %(accuracy*100))\n",
    "print(classification_report(Y_test, Y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model attempts to improve upon that used in Rathi et al. (2018) by adding dropout layers at 40% between Conv2D and MaxPooling Layers and only implements 2 Conv2D layers instead of 3. This is mainly to combat overfitting and improve generalization and accuracy (as well as accuracy across all classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fish_classification_nn():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=(100, 100, 4)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.MaxPooling2D((5, 5), padding = 'same'))\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (5, 5), activation='relu'))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.MaxPooling2D((5, 5), padding = 'same'))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(200, activation='relu'))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(23, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#Compile and train the model\n",
    "fish_classification = fish_classification_nn()\n",
    "fish_classification.summary()\n",
    "\n",
    "fish_classification.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "fish_classification_history = fish_classification.fit(X_train, Y_train, validation_data=(X_validation, Y_validation),\n",
    "                                                      epochs= 100 , batch_size= 50, shuffle=True)\n",
    "\n",
    "\n",
    "# Plotting the accuracy and loss for the training and validation data (vs. EPOCH)\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.title('The Accuracy Vs. Epoch No.', fontsize= 14)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(fish_classification_history.history['val_accuracy'])\n",
    "plt.plot(fish_classification_history.history['accuracy'])\n",
    "plt.ylabel('Accuracy', fontsize= 14)\n",
    "plt.xlabel('Epoch No.', fontsize= 14)\n",
    "plt.title('The Accuracy Vs. Epoch No.', fontsize= 14)\n",
    "plt.legend(('Validation Set Accuracy','Training Set Accuracy'), fontsize= 14, loc=4)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(fish_classification_history.history['val_loss'])\n",
    "plt.plot(fish_classification_history.history['loss'])\n",
    "plt.ylabel('Loss', fontsize= 14)\n",
    "plt.xlabel('Epoch No.', fontsize= 14)\n",
    "plt.title('The Loss Vs. Epoch No.', fontsize= 14)\n",
    "plt.legend(('Validation Set Loss','Training Set Loss'), fontsize= 14, loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on the testing dataset\n",
    "We evaluate the model on the testing dataset as well as generate the confusion matrix (normalized) and classification report (with precision, recall, f1-score, and support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# Predicting the classes of the testing dataset\n",
    "Y_pred = fish_classification.predict_classes(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "con_mat = confusion_matrix(Y_test,Y_pred)\n",
    "\n",
    "# normalize result from 0 to 1:\n",
    "sum_of_classes = con_mat.sum(axis=1)\n",
    "con_mat_norm = np.zeros((23,23))\n",
    "for i in range(con_mat.shape[0]):\n",
    "    \n",
    "    if sum_of_classes[i] != 0:\n",
    "        con_mat_norm[i,:] = np.around(con_mat.astype('float')[i,:] / sum_of_classes[i], decimals=2) \n",
    "# the above code rounds to the first 2 decimals and then divides by the sum of every col \n",
    "\n",
    "classes=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]\n",
    "\n",
    "con_mat_df = pd.DataFrame(con_mat_norm, index = classes, columns = classes)\n",
    " \n",
    "figure = plt.figure(figsize=(15,10))\n",
    "sb.set(font_scale = 1.3)\n",
    "sb.heatmap(con_mat_norm, annot=True, cmap='inferno', annot_kws={\"size\": 12},  square=True)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()\n",
    "\n",
    "## Test set accuracy\n",
    "accuracy = 1 - np.count_nonzero((Y_test[:,0] - Y_pred))/len(Y_pred)\n",
    "print(\"The Accuracy for the Improved Model is: %.2f%%\" %(accuracy*100))\n",
    "print(classification_report(Y_test, Y_pred, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
